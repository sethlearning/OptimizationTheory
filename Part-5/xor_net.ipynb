{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 0 1]\n",
      " [1 1 0]\n",
      " [1 1 1]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# программа расчета нейросети, реализующей функцию XOR\n",
    "import numpy as np\n",
    "\n",
    "# Обучающая выборка\n",
    "X = np.array([ [1,0,0],[1,0,1],[1,1,0],[1,1,1] ])\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- Моделирование классификатора XOR -------------\n",
    "# логистическая функция активации\n",
    "def sigmoidfun(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmderiv(z):\n",
    "    return sigmoidfun(z) * (1 - sigmoidfun(z))\n",
    "\n",
    "def sigmoideriv(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "np.random.seed(1)\n",
    "s0 = 2 * np.random.random((2, 3)) - 1\n",
    "s1 = 2 * np.random.random((1, 3)) - 1\n",
    "\n",
    "def xor_nnet(x, syn0=s0, syn1=s1):\n",
    "    a1 = x.T\n",
    "    a2 = sigmoidfun(np.dot(syn0, a1))\n",
    "    if len(a2.shape) > 1:\n",
    "        ones = np.ones(shape=(1, a2.shape[1]))\n",
    "        a20 = np.vstack((ones, a2))\n",
    "    else:\n",
    "        a20 = np.array([1, a2[0], a2[1]])\n",
    "    a3 = sigmoidfun(np.dot(syn1, a20))\n",
    "    return a1, a20, a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ классификатора для [1 0 0] = [-0.00764414]\n",
      "Ответ классификатора для [1 0 1] = [0.00715279]\n",
      "Ответ классификатора для [1 1 0] = [0.00715279]\n",
      "Ответ классификатора для [1 1 1] = [-0.00764414]\n"
     ]
    }
   ],
   "source": [
    "# -------------- Моделирование классификатора XOR -------------\n",
    "# Задаем веса для классификатора, реализующего функцию XOR\n",
    "syn0 = np.array([ [-5,-10,10],[-5,10,-10] ])\n",
    "syn1 = np.array([ [-5,10,10] ])\n",
    "\n",
    "ynet = np.array([[0.0, 0.0, 0.0, 0.0]]).T\n",
    "err = ynet\n",
    "for i in range(len(X)):\n",
    "    x = X[i]\n",
    "    a1,a2,ynet[i] = xor_nnet(x, syn0, syn1)\n",
    "    err[i] = y[i] - ynet[i]\n",
    "    print(\"Ответ классификатора для\", x, \"=\", ynet[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE= [5.47975957e-05]\n"
     ]
    }
   ],
   "source": [
    "# --------------- анализ ошибок классификации -----------------\n",
    "# СРЕДНЕКВАДРАТИЧЕСКАЯ ОШИБКА (MSE)\n",
    "print('MSE=', sum(err*err)/len(err))\n",
    "# print('MSE=', np.mean(err*err))\n",
    "\n",
    "# кроссэнтропия распределений (log-loss)\n",
    "def plogp(p):\n",
    "    if abs(p) < 1e-12:\n",
    "        return 0\n",
    "    return p*np.log2(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18096335 0.16557691 0.14980345 0.14153966]]\n",
      "Error:0.5017806632374381\n",
      "Error:0.4958867934391441\n",
      "Error:0.3446072879383419\n",
      "Error:0.09127435453125382\n",
      "Error:0.05664042134229113\n",
      "Error:0.043883382134332585\n",
      "Error:0.036893905686122316\n",
      "Error:0.03235945458778854\n",
      "Error:0.029125933032138458\n",
      "Error:0.026676011464766485\n",
      "Error:0.024739776387334697\n",
      "Error:0.023161204339646607\n",
      "Error:0.021843162483658703\n",
      "Error:0.020721678807102403\n",
      "Error:0.01975271525426613\n",
      "Error:0.01890485393499014\n",
      "Error:0.018155010580695505\n",
      "Error:0.017485799722054992\n",
      "Error:0.01688384952268858\n",
      "Error:0.016338687103310334\n",
      "[[ -4.48638541 -10.51846703   9.74164621]]\n",
      "[[-7.00981282  4.58522782  4.55025869]\n",
      " [-2.98183124  6.76632391  6.60203154]]\n",
      "[[0.01753602 0.98482712 0.98498406 0.01564307]]\n"
     ]
    }
   ],
   "source": [
    "# ------- попробуем обучить нейросеть простым град-м спуском ---------\n",
    "# Попробуем обучить нейросеть XOR с помощью оптимизации\n",
    "# случайно инициализируем веса, в среднем - 0\n",
    "np.random.seed(3)\n",
    "syn0 = 2*np.random.random((2,3)) - 1\n",
    "syn1 = 2*np.random.random((1,3)) - 1\n",
    "\n",
    "a1,a2,ynet = xor_nnet(X, syn0, syn1)\n",
    "print(ynet)\n",
    "# learning coef\n",
    "alpha = 0.01\n",
    "\n",
    "# Попробуем обучить нейросеть XOR\n",
    "for j in range(600000):\n",
    "    # проходим вперёд по слоям 0, 1 и 2\n",
    "    a10,a20,a3 = xor_nnet(X, syn0, syn1)\n",
    "\n",
    "    # как сильно мы ошиблись относительно нужной величины?\n",
    "    a3_error = y - a3.T\n",
    "\n",
    "    # рапространим ошибку на выходе 3-го слоя на вход слоя\n",
    "    z3_delta = a3_error * sigmoideriv(a3.T)\n",
    "\n",
    "    # распространим ошибки z3 на ошибки выходов a2 (входы для a3)\n",
    "    a20_error = np.dot(syn1.T, z3_delta.T)\n",
    "    # уберем 0-ю строку, содержащую добавленный +1\n",
    "    a2 = a20[1:]\n",
    "    a2_error = a20_error[1:]\n",
    "\n",
    "    # рапространим ошибку на выходе 2-го слоя на вход слоя\n",
    "    z2_delta = a2_error * sigmoideriv(a2)\n",
    "\n",
    "    # рассчитаем дельта весов каждого слоя,\n",
    "    # используя ошибки на входе (l+1) слоя и выходы l-го слоя\n",
    "    syn1_delta = np.dot(z3_delta.T, a20.T)\n",
    "    syn0_delta = np.dot(z2_delta, a10.T)\n",
    "\n",
    "    syn1 += alpha* syn1_delta\n",
    "    syn0 += alpha* syn0_delta\n",
    "\n",
    "    if (j % 30000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(a3_error))))\n",
    "\n",
    "print(syn1)\n",
    "print(syn0)\n",
    "a1,a2,ynet = xor_nnet(X, syn0, syn1)\n",
    "print(ynet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.6087197679120167\n",
      "Error:0.4817883120080427\n",
      "Error:0.17647421410589734\n",
      "Error:0.061995343720827024\n",
      "Error:0.04388113548099668\n",
      "Error:0.035063979375143675\n",
      "Error:0.030010002413792883\n",
      "Error:0.027850712051608162\n",
      "Error:0.024821579569469083\n",
      "Error:0.022899323304130175\n",
      "Error:0.020637568355052968\n",
      "Error:0.020387557586109013\n",
      "[[ -4.31594635 -10.19845644   9.41363496]]\n",
      "[[-6.81597651  4.46157227  4.42166219]\n",
      " [-2.92654715  6.69726717  6.5037521 ]]\n",
      "[[1 1 1 1]\n",
      " [0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "[[1.         1.         1.         1.        ]\n",
      " [0.00109492 0.08360729 0.08671634 0.88767986]\n",
      " [0.05085674 0.97280644 0.97748321 0.9999655 ]]\n",
      "[[0.02086971 0.98181906 0.98203764 0.01878744]]\n"
     ]
    }
   ],
   "source": [
    "# ------- попробуем обучить нейросеть стохастическим градиентом ---------\n",
    "# случайно инициализируем веса, в среднем - 0\n",
    "np.random.seed(3)\n",
    "syn0 = 2*np.random.random((2,3)) - 1\n",
    "syn1 = 2*np.random.random((1,3)) - 1\n",
    "\n",
    "# learning coef\n",
    "alpha = 0.01\n",
    "# pack size\n",
    "pack_size = 3\n",
    "\n",
    "# Попробуем обучить нейросеть XOR\n",
    "for j in range(600000):\n",
    "    # проходим вперёд по слоям 0, 1 и 2\n",
    "    a10,a20,a3 = xor_nnet(X, syn0, syn1)\n",
    "    # выбираем случайно pack_size объектов из обучающей выборки\n",
    "    nobjs = np.random.choice(list(range(len(X))), pack_size, replace=False)\n",
    "    # фильтруем нужные объекты и ответы\n",
    "    a10 = a10[:, nobjs]\n",
    "    a20 = a20[:, nobjs]\n",
    "    a3 = a3[:, nobjs]\n",
    "    yp = y[nobjs]\n",
    "\n",
    "    # как сильно мы ошиблись относительно нужной величины?\n",
    "    a3_error = yp - a3.T\n",
    "\n",
    "    # рапространим ошибку на выходе 3-го слоя на вход слоя\n",
    "    z3_delta = a3_error * sigmoideriv(a3.T)\n",
    "\n",
    "    # распространим ошибки z3 на ошибки выходов a2 (входы для a3)\n",
    "    a20_error = np.dot(syn1.T, z3_delta.T)\n",
    "    # уберем 0-ю строку, содержащую добавленный +1\n",
    "    a2 = a20[1:]\n",
    "    a2_error = a20_error[1:]\n",
    "\n",
    "    # рапространим ошибку на выходе 2-го слоя на вход слоя\n",
    "    z2_delta = a2_error * sigmoideriv(a2)\n",
    "\n",
    "    # рассчитаем дельта весов каждого слоя,\n",
    "    # используя ошибки на входе (l+1) слоя и выходы l-го слоя\n",
    "    syn1_delta = np.dot(z3_delta.T, a20.T)\n",
    "    syn0_delta = np.dot(z2_delta, a10.T)\n",
    "\n",
    "    syn1 += alpha* syn1_delta\n",
    "    syn0 += alpha* syn0_delta\n",
    "\n",
    "    if (j % 50000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(a3_error))))\n",
    "\n",
    "print(syn1)\n",
    "print(syn0)\n",
    "a1,a2,ynet = xor_nnet(X, syn0, syn1)\n",
    "print(a1)\n",
    "print(a2)\n",
    "print(ynet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- реализация нейросети XOR с помощью KERAS\n",
    "# import the necessary packages\n",
    "from keras.optimizers import SGD, Adam, Nadam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# imports used to build the deep learning model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import BatchNormalization, Dropout, Lambda\n",
    "from keras.models import Model\n",
    "\n",
    "# from keras. import binary_crossentropy\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graph_training_history(history):\n",
    "    plt.figure(1)\n",
    "\n",
    "    # summarize history for accuracy\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    # summarize history for loss\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_XORnnet(nx = 2):\n",
    "    # Input\n",
    "    a1 = Input(shape=(nx,))\n",
    "    a2 = Dense(2, activation='sigmoid')(a1)\n",
    "    a3 = Dense(1, activation='sigmoid')(a2)\n",
    "\n",
    "    classificator = Model(a1, a3, name=\"XORnnet\")\n",
    "    # Return the constructed network architecture\n",
    "    return classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"XORnnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 6         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_XORnnet(nx = 2)\n",
    "# Посмотрим на число параметров\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Building and compiling the DNet model...\n"
     ]
    }
   ],
   "source": [
    "# Зададим основные параметры для обучения\n",
    "batch_size = 4\n",
    "Xb = X[:,1:]\n",
    "yb = y\n",
    "\n",
    "# Build and Compile the model\n",
    "print(\"[INFO] Building and compiling the DNet model...\")\n",
    "# opt = SGD(lr=0.0002, momentum=0.0001, decay=0.00001, nesterov=True)\n",
    "opt = Adam(lr=0.01)\n",
    "# opt = Nadam(lr=0.0001)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the argument whether to train the model\n",
    "print(\"[INFO] Training the model...\")\n",
    "history = model.fit(Xb, yb,\n",
    "                    batch_size= batch_size,\n",
    "                    epochs=500,\n",
    "                    shuffle= True,\n",
    "                    #validation_split= 0.3,\n",
    "                    validation_data=(Xb, yb),\n",
    "                    verbose=1)\n",
    "\n",
    "# Visualize the training history\n",
    "graph_training_history(history)\n",
    "\n",
    "# Training of the model is now complete\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Evaluating the model...\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1356 - accuracy: 1.0000\n",
      "[INFO] CONTROL accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Use the test data to evaluate the model\n",
    "# check it on ctrldata - проверяем точность модели на контрольной выборке\n",
    "#ctrlLabels_softmax = np_utils.to_categorical(ctrlLabels, n_classes)\n",
    "print(\"[INFO] Evaluating the model...\")\n",
    "# (loss, accuracy) = model.evaluate(ctrlData, ctrlLabels_softmax, verbose=1, batch_size= batch_size)\n",
    "(loss, accuracy) = model.evaluate(Xb, yb, verbose=1, batch_size= batch_size)\n",
    "print(\"[INFO] CONTROL accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
